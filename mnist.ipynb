{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "init_channels = 2\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=init_channels, \n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, \n",
    "                               out_channels=2*init_channels, \n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.fc = nn.Linear(in_features=2*init_channels, out_features=10)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                # m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.avg_pool2d(x, 8)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# SGD Training\n",
    "def train(train_queue, net, criterion, optimizer):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for step, (inputs, targets) in enumerate(train_queue):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if step % report_freq == 0:\n",
    "            print('train %03d %e %f' %(step, train_loss/total, 100.*correct/total))\n",
    "\n",
    "    print('train acc %f' %(100. * correct / total))\n",
    "\n",
    "    return train_loss/total, 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def infer(valid_queue, net, criterion):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (inputs, targets) in enumerate(valid_queue):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if step % report_freq == 0:\n",
    "                print('valid %03d %e %f' % (step, test_loss/total, 100.*correct/total))\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print('valid acc %f' % (acc))\n",
    "\n",
    "    return test_loss/total, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "LEARNING_RATE = 0.025\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECEAY = 3e-4\n",
    "MIN_LEARNING_RATE = 0 \n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "seed = 0\n",
    "device = torch.device(\"cpu\")\n",
    "report_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ------------- main routine ------------------ #\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Net().to(device)\n",
    "    n_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('# number of trainable parameters = {}'.format(n_params_trainable))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # --------- SGD optimization ------------------\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.SGD(parameters,\n",
    "                          lr=LEARNING_RATE,\n",
    "                          momentum=MOMENTUM,\n",
    "                          weight_decay=WEIGHT_DECEAY)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                           N_EPOCHS, \n",
    "                                                           eta_min=MIN_LEARNING_RATE)\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        train(train_loader, model, criterion, optimizer)\n",
    "        infer(test_loader, model, criterion)\n",
    "    \n",
    "    utils.save(model, 'weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of trainable parameters = 140\n",
      "Epoch 1\n",
      "train 000 2.155834e-02 6.250000\n",
      "train 100 1.802606e-02 11.270111\n",
      "train 200 1.793263e-02 12.216262\n",
      "train 300 1.779825e-02 14.926806\n",
      "train 400 1.750631e-02 17.265430\n",
      "train acc 18.901667\n",
      "valid 000 1.463872e-02 35.156250\n",
      "valid acc 35.280000\n",
      "Epoch 2\n",
      "train 000 1.481102e-02 34.375000\n",
      "train 100 1.401875e-02 34.220297\n",
      "train 200 1.361530e-02 36.046331\n",
      "train 300 1.320089e-02 38.156665\n",
      "train 400 1.287802e-02 39.500857\n",
      "train acc 40.216667\n",
      "valid 000 1.134049e-02 50.000000\n",
      "valid acc 45.520000\n",
      "Epoch 3\n",
      "train 000 1.141813e-02 39.843750\n",
      "train 100 1.130199e-02 46.287129\n",
      "train 200 1.113663e-02 47.341418\n",
      "train 300 1.098234e-02 48.229859\n",
      "train 400 1.084582e-02 49.207060\n",
      "train acc 49.711667\n",
      "valid 000 1.003930e-02 49.218750\n",
      "valid acc 55.530000\n",
      "Epoch 4\n",
      "train 000 9.972746e-03 59.375000\n",
      "train 100 1.005764e-02 53.805693\n",
      "train 200 9.980417e-03 54.158893\n",
      "train 300 9.897719e-03 54.666736\n",
      "train 400 9.813182e-03 55.121961\n",
      "train acc 55.605000\n",
      "valid 000 9.335888e-03 53.906250\n",
      "valid acc 55.240000\n",
      "Epoch 5\n",
      "train 000 9.373780e-03 53.906250\n",
      "train 100 9.371539e-03 57.773824\n",
      "train 200 9.321578e-03 57.839708\n",
      "train 300 9.283962e-03 58.253738\n",
      "train 400 9.217767e-03 58.628819\n",
      "train acc 58.820000\n",
      "valid 000 8.252548e-03 62.500000\n",
      "valid acc 60.480000\n",
      "Epoch 6\n",
      "train 000 1.093265e-02 52.343750\n",
      "train 100 8.890945e-03 60.775062\n",
      "train 200 8.862995e-03 61.096859\n",
      "train 300 8.822359e-03 61.025748\n",
      "train 400 8.736663e-03 61.407029\n",
      "train acc 61.478333\n",
      "valid 000 7.878100e-03 60.156250\n",
      "valid acc 64.210000\n",
      "Epoch 7\n",
      "train 000 9.855106e-03 61.718750\n",
      "train 100 8.478472e-03 62.654703\n",
      "train 200 8.338591e-03 63.378420\n",
      "train 300 8.300070e-03 63.509655\n",
      "train 400 8.266465e-03 63.624143\n",
      "train acc 63.808333\n",
      "valid 000 7.346843e-03 65.625000\n",
      "valid acc 66.810000\n",
      "Epoch 8\n",
      "train 000 6.689639e-03 73.437500\n",
      "train 100 7.968223e-03 65.292389\n",
      "train 200 8.022190e-03 64.882618\n",
      "train 300 7.993273e-03 64.843750\n",
      "train 400 7.983927e-03 64.896353\n",
      "train acc 65.025000\n",
      "valid 000 7.313921e-03 63.281250\n",
      "valid acc 67.040000\n",
      "Epoch 9\n",
      "train 000 8.130407e-03 62.500000\n",
      "train 100 7.794249e-03 65.439356\n",
      "train 200 7.781300e-03 65.360697\n",
      "train 300 7.715123e-03 65.876765\n",
      "train 400 7.750042e-03 65.804239\n",
      "train acc 65.918333\n",
      "valid 000 6.834151e-03 71.093750\n",
      "valid acc 67.450000\n",
      "Epoch 10\n",
      "train 000 7.200603e-03 67.187500\n",
      "train 100 7.705227e-03 66.274752\n",
      "train 200 7.649823e-03 66.499534\n",
      "train 300 7.634381e-03 66.707330\n",
      "train 400 7.633015e-03 66.682902\n",
      "train acc 66.581667\n",
      "valid 000 6.780538e-03 71.093750\n",
      "valid acc 68.090000\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pre-trained model\n",
    "model_pretrained = Net().to(device)\n",
    "utils.load(model_pretrained, 'weights.pt')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "valid 000 1.116145e-03 94.531250\n",
      "valid 100 1.481796e-03 94.129022\n",
      "valid 200 1.483359e-03 94.115361\n",
      "valid 300 1.493303e-03 94.123754\n",
      "valid 400 1.539796e-03 93.972101\n",
      "valid acc 93.935000\n",
      "valid 000 1.692700e-03 92.968750\n",
      "valid acc 94.110000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0014927162315696478, 94.11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the model \n",
    "print(model_pretrained)\n",
    "infer(train_loader, model_pretrained, criterion)\n",
    "infer(test_loader, model_pretrained, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace last fc layer with identity to simply output features \n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1146e+00, 0.0000e+00, 0.0000e+00, 1.2190e-03, 7.1843e-02, 2.2037e+00,\n",
      "         1.2682e+01, 6.8865e-01, 1.0824e+01, 5.6788e-01, 3.3159e-01, 1.7093e+00,\n",
      "         0.0000e+00, 9.1272e-01, 9.4400e-02, 6.5170e-02]])\n",
      "[1.1146223545074463, 0.0, 0.0, 0.001218983088620007, 0.07184304296970367, 2.203676223754883, 12.682160377502441, 0.6886472702026367, 10.82374382019043, 0.5678792595863342, 0.331586629152298, 1.7092713117599487, 0.0, 0.9127167463302612, 0.09439995884895325, 0.06517039239406586]\n"
     ]
    }
   ],
   "source": [
    "model_pretrained.fc = Identity()\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "output = model_pretrained(x)\n",
    "print(output.data)\n",
    "features = output.data.numpy().tolist()\n",
    "features_str = ','.join(map(str, features)) \n",
    "print(features_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataset for yashesh to try\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False)\n",
    "\n",
    "model_pretrained.eval()\n",
    "\n",
    "train_data_set = []\n",
    "with torch.no_grad():\n",
    "    for step, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model_pretrained(inputs)\n",
    "#         print(targets.item())\n",
    "#         print(outputs)\n",
    "        features = (outputs.data.numpy().tolist())[0]\n",
    "        features.append(targets.item())\n",
    "        features_str = ','.join(map(str, features)) \n",
    "        train_data_set.append(features_str)\n",
    "        \n",
    "with open('mnist_train_data.csv', 'w') as handle:\n",
    "    for line in train_data_set:\n",
    "        handle.write(line)\n",
    "        handle.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare testing dataset for yashesh to try\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False)\n",
    "\n",
    "model_pretrained.eval()\n",
    "\n",
    "test_data_set = []\n",
    "with torch.no_grad():\n",
    "    for step, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model_pretrained(inputs)\n",
    "#         print(targets.item())\n",
    "#         print(outputs)\n",
    "        features = (outputs.data.numpy().tolist())[0]\n",
    "        features.append(targets.item())\n",
    "        features_str = ','.join(map(str, features)) \n",
    "        test_data_set.append(features_str)\n",
    "        \n",
    "with open('mnist_test_data.csv', 'w') as handle:\n",
    "    for line in test_data_set:\n",
    "        handle.write(line)\n",
    "        handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = ','.join(map(str, myList)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions needed for evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def random_weights(dim, mu=0, sigma=1):\n",
    "    # gaussian\n",
    "    return np.random.normal(mu, sigma, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_parameters(model, params_to_load):\n",
    "    lb = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        ub = lb + param.nelement()\n",
    "        layer_size = tuple(param.size())\n",
    "        param.data = (torch.from_numpy(params_to_load[lb:ub].reshape(layer_size))).type(torch.FloatTensor)\n",
    "        lb += param.nelement()\n",
    "    assert ub == len(params_to_load)\n",
    "    return  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_combination(iterable, sample_size):\n",
    "    \"\"\"Random selection from itertools.combinations(iterable, r).\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    \n",
    "    indices = sorted(random.sample(range(n), sample_size))\n",
    "    \n",
    "    return tuple(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# polynomial mutation from pymoo package\n",
    "from pymoo.model.mutation import Mutation\n",
    "\n",
    "class PolynomialMutation(Mutation):\n",
    "    def __init__(self, eta_mut, prob_mut=None, xl=-5, xu=5):\n",
    "        super().__init__()\n",
    "        # artificial lower&upper bound for weight\n",
    "        self.eta_mut = float(eta_mut)\n",
    "        self.xl = xl\n",
    "        self.xu = xu\n",
    "        if prob_mut is not None:\n",
    "            self.prob_mut = float(prob_mut)\n",
    "        else:\n",
    "            self.prob_mut = None\n",
    "        \n",
    "    def do(self, X):\n",
    "        \n",
    "        Y = np.full(X.shape, np.inf)\n",
    "\n",
    "        if self.prob_mut is None:\n",
    "            self.prob_mut = 1.0 / problem.n_var\n",
    "\n",
    "        do_mutation = np.random.rand(X.shape[0]) < self.prob_mut\n",
    "\n",
    "        Y[:] = X\n",
    "\n",
    "        xl = np.repeat(self.xl, X.shape[0], axis=0)[do_mutation]\n",
    "        xu = np.repeat(self.xu, X.shape[0], axis=0)[do_mutation]\n",
    "    \n",
    "        X = X[do_mutation]\n",
    "\n",
    "        delta1 = (X - xl) / (xu - xl)\n",
    "        delta2 = (xu - X) / (xu - xl)\n",
    "\n",
    "        mut_pow = 1.0 / (self.eta_mut + 1.0)\n",
    "\n",
    "        rand = np.random.rand(X.shape[0])\n",
    "        mask = rand <= 0.5\n",
    "        mask_not = np.logical_not(mask)\n",
    "\n",
    "        deltaq = np.zeros(X.shape)\n",
    "\n",
    "        xy = 1.0 - delta1\n",
    "        val = 2.0 * rand + (1.0 - 2.0 * rand) * (np.power(xy, (self.eta_mut + 1.0)))\n",
    "        d = np.power(val, mut_pow) - 1.0\n",
    "        deltaq[mask] = d[mask]\n",
    "\n",
    "        xy = 1.0 - delta2\n",
    "        val = 2.0 * (1.0 - rand) + 2.0 * (rand - 0.5) * (np.power(xy, (self.eta_mut + 1.0)))\n",
    "        d = 1.0 - (np.power(val, mut_pow))\n",
    "        deltaq[mask_not] = d[mask_not]\n",
    "\n",
    "        # mutated values\n",
    "        _Y = X + deltaq * (xu - xl)\n",
    "\n",
    "        # back in bounds if necessary (floating point issues)\n",
    "        _Y[_Y < xl] = xl[_Y < xl]\n",
    "        _Y[_Y > xu] = xu[_Y > xu]\n",
    "\n",
    "        # set the values for output\n",
    "        Y[do_mutation] = _Y\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate(pop, train_queue, criterion):\n",
    "    # every individual in population will only be evaluated on one mini-batch\n",
    "    # assuming mini-batch size = total_num_training_data / population size\n",
    "    pop_eval = []\n",
    "    best_loss = np.inf\n",
    "    for step, (inputs, targets) in enumerate(train_queue):\n",
    "        if step < len(pop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model = Net().to(device)\n",
    "            load_parameters(model, pop[step][1])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets).item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct = predicted.eq(targets).sum().item()\n",
    "            acc = 100.*correct/len(inputs)\n",
    "            pop_eval.append((loss, pop[step][1], acc))\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return pop_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evo_main(generations=3000,\n",
    "             population_size=100,\n",
    "             tournament_size=10,\n",
    "             p_mut=0.05, eta_m=30):\n",
    "    # simply apply gaussian noise (zero mean and std 1) to all parameters\n",
    "    # ------------- main routine ------------------ #\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    batch_size = int(50000/population_size)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = Net()\n",
    "    n_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('# number of trainable parameters = {}'.format(n_params_trainable))\n",
    "    del model\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # initialization\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        weights = random_weights(n_params_trainable, sigma=0.01)\n",
    "        population.append((np.inf, weights, 0))\n",
    "    \n",
    "    # evaluation\n",
    "    population = evaluate(population, train_loader, criterion)\n",
    "    \n",
    "    elite_idx = np.argmin([x[0] for x in population])\n",
    "    print('train %03d %e %e %f'% (0, p_mut, \n",
    "                                  population[elite_idx][0],\n",
    "                                  population[elite_idx][-1]))\n",
    "    \n",
    "    # main loop of evolution\n",
    "    for gen in range(1, generations+1):\n",
    "        sample = random_combination(population, tournament_size)\n",
    "        # best from the sample becomes parent\n",
    "        tmp = sorted(sample, key=lambda i: i[0])\n",
    "                \n",
    "        winner, loser = tmp[0], tmp[-1]\n",
    "        \n",
    "        # update mutation probability - linear drop \n",
    "        p_mut = p_mut - (p_mut - 1/n_params_trainable)/generations\n",
    "        child = [(np.inf, PolynomialMutation(eta_mut=eta_m, \n",
    "                                             prob_mut=p_mut,\n",
    "                                             xl=-2, xu=2).do(winner[1]))]\n",
    "        child = evaluate(child, train_loader, criterion)\n",
    "        \n",
    "        # replace loser in population with child\n",
    "        remove_idx = [i for i in range(len(population)) \n",
    "                      if np.all(population[i][1] == loser[1])][0]\n",
    "                \n",
    "        population.pop(remove_idx)\n",
    "        population += child\n",
    "        if gen % 100 == 0:\n",
    "            elite_idx = np.argmin([x[0] for x in population])\n",
    "            print('train %03d %e %e %f'% (0, p_mut, \n",
    "                                          population[elite_idx][0],\n",
    "                                          population[elite_idx][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of trainable parameters = 1394\n",
      "train 000 1.000000e+00 2.302330e+00 10.120000\n",
      "train 000 9.672342e-01 2.299078e+00 12.160000\n",
      "train 000 9.355429e-01 2.293097e+00 10.080000\n",
      "train 000 9.048906e-01 2.282295e+00 10.160000\n",
      "train 000 8.752434e-01 2.273449e+00 10.360000\n",
      "train 000 8.465684e-01 2.273449e+00 10.360000\n",
      "train 000 8.188335e-01 2.273449e+00 10.360000\n",
      "train 000 7.920081e-01 2.273449e+00 10.360000\n",
      "train 000 7.660622e-01 2.272149e+00 19.440000\n",
      "train 000 7.409671e-01 2.272149e+00 19.440000\n",
      "train 000 7.166949e-01 2.264886e+00 12.400000\n",
      "train 000 6.932185e-01 2.257338e+00 18.240000\n",
      "train 000 6.705119e-01 2.257338e+00 18.240000\n",
      "train 000 6.485498e-01 2.257338e+00 18.240000\n",
      "train 000 6.273078e-01 2.247084e+00 12.720000\n",
      "train 000 6.067624e-01 2.231838e+00 18.320000\n",
      "train 000 5.868906e-01 2.225982e+00 20.240000\n",
      "train 000 5.676704e-01 2.212746e+00 14.240000\n",
      "train 000 5.490804e-01 2.212746e+00 14.240000\n",
      "train 000 5.311000e-01 2.212746e+00 14.240000\n",
      "train 000 5.137092e-01 2.212746e+00 14.240000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-13f38b0e12c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m evo_main(generations=3000, \n\u001b[1;32m      2\u001b[0m          \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          tournament_size=5, p_mut=1.0, eta_m=50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-38fce1768fc7>\u001b[0m in \u001b[0;36mevo_main\u001b[0;34m(generations, population_size, tournament_size, p_mut, eta_m)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                              \u001b[0mprob_mut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_mut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                              xl=-2, xu=2).do(winner[1]))]\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# replace loser in population with child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-19f5fdbaba62>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(pop, train_queue, criterion)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpop_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m     \"\"\"\n\u001b[0;32m-> 2508\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "evo_main(generations=3000, \n",
    "         population_size=20, \n",
    "         tournament_size=5, p_mut=1.0, eta_m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([8, 1, 3, 3])\n",
      "(72, 1)\n",
      "conv2.weight torch.Size([16, 8, 3, 3])\n",
      "(1152, 1)\n",
      "fc.weight torch.Size([10, 16])\n",
      "(160, 1)\n",
      "fc.bias torch.Size([10])\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# loop over model to print weights \n",
    "model = Net()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "    param_vector = param.view(param.nelement(), -1).data.numpy()\n",
    "    print(param_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394\n",
      "(1394,)\n",
      "lb = 0\n",
      "ub = 72\n",
      "lb = 72\n",
      "ub = 1224\n",
      "lb = 1224\n",
      "ub = 1384\n",
      "lb = 1384\n",
      "ub = 1394\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "n_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "init_guess = random_weights(n_params_trainable)\n",
    "\n",
    "lb = 0\n",
    "for name, param in model.named_parameters():\n",
    "    ub = lb + param.nelement()\n",
    "    print('lb = {}'.format(lb))\n",
    "    print('ub = {}'.format(ub))\n",
    "    layer_size = tuple(param.size())\n",
    "    param.data = torch.from_numpy(init_guess[lb:ub].reshape(layer_size))\n",
    "    lb += param.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
