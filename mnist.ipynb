{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "init_channels = 8\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=init_channels, \n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels=init_channels, \n",
    "                               out_channels=2*init_channels, \n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.fc = nn.Linear(in_features=2*init_channels, out_features=10)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                # m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.avg_pool2d(x, 8)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# SGD Training\n",
    "def train(train_queue, net, criterion, optimizer):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for step, (inputs, targets) in enumerate(train_queue):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if step % report_freq == 0:\n",
    "            print('train %03d %e %f' %(step, train_loss/total, 100.*correct/total))\n",
    "\n",
    "    print('train acc %f' %(100. * correct / total))\n",
    "\n",
    "    return train_loss/total, 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def infer(valid_queue, net, criterion):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (inputs, targets) in enumerate(valid_queue):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if step % report_freq == 0:\n",
    "                print('valid %03d %e %f' % (step, test_loss/total, 100.*correct/total))\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    print('valid acc %f' % (acc))\n",
    "\n",
    "    return test_loss/total, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "LEARNING_RATE = 0.025\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECEAY = 3e-4\n",
    "MIN_LEARNING_RATE = 0 \n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "seed = 0\n",
    "device = torch.device(\"cpu\")\n",
    "report_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ------------- main routine ------------------ #\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Net().to(device)\n",
    "    n_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('# number of trainable parameters = {}'.format(n_params_trainable))\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # --------- SGD optimization ------------------\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.SGD(parameters,\n",
    "                          lr=LEARNING_RATE,\n",
    "                          momentum=MOMENTUM,\n",
    "                          weight_decay=WEIGHT_DECEAY)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                           N_EPOCHS, \n",
    "                                                           eta_min=MIN_LEARNING_RATE)\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        train(train_loader, model, criterion, optimizer)\n",
    "        infer(test_loader, model, criterion)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of trainable parameters = 1394\n",
      "Epoch 1\n",
      "train 000 1.901529e-02 14.062500\n",
      "train 100 1.633249e-02 24.860767\n",
      "train 200 1.414824e-02 34.619869\n",
      "train 300 1.241141e-02 43.695494\n",
      "train 400 1.100943e-02 50.617597\n",
      "train acc 54.280000\n",
      "valid 000 4.889995e-03 82.031250\n",
      "valid acc 79.400000\n",
      "Epoch 2\n",
      "train 000 5.162074e-03 82.812500\n",
      "train 100 5.026588e-03 79.347153\n",
      "train 200 4.750759e-03 80.597015\n",
      "train 300 4.470814e-03 81.852159\n",
      "train 400 4.245703e-03 82.868999\n",
      "train acc 83.263333\n",
      "valid 000 3.550611e-03 88.281250\n",
      "valid acc 84.000000\n",
      "Epoch 3\n",
      "train 000 3.548364e-03 80.468750\n",
      "train 100 3.072465e-03 87.871287\n",
      "train 200 3.165774e-03 87.348414\n",
      "train 300 3.090475e-03 87.705046\n",
      "train 400 3.024134e-03 87.895496\n",
      "train acc 88.013333\n",
      "valid 000 2.500935e-03 90.625000\n",
      "valid acc 90.130000\n",
      "Epoch 4\n",
      "train 000 3.501358e-03 84.375000\n",
      "train 100 2.540852e-03 90.052599\n",
      "train 200 2.509796e-03 90.123601\n",
      "train 300 2.456327e-03 90.373235\n",
      "train 400 2.448742e-03 90.348348\n",
      "train acc 90.335000\n",
      "valid 000 2.234310e-03 92.968750\n",
      "valid acc 90.950000\n",
      "Epoch 5\n",
      "train 000 2.229103e-03 89.843750\n",
      "train 100 2.384920e-03 90.400681\n",
      "train 200 2.268826e-03 91.056437\n",
      "train 300 2.267010e-03 90.985777\n",
      "train 400 2.246101e-03 91.078943\n",
      "train acc 91.261667\n",
      "valid 000 1.880304e-03 91.406250\n",
      "valid acc 92.060000\n",
      "Epoch 6\n",
      "train 000 1.961342e-03 92.187500\n",
      "train 100 2.168714e-03 91.831683\n",
      "train 200 2.081822e-03 92.055348\n",
      "train 300 2.041589e-03 92.169331\n",
      "train 400 2.002528e-03 92.261534\n",
      "train acc 92.145000\n",
      "valid 000 2.004213e-03 92.187500\n",
      "valid acc 93.020000\n",
      "Epoch 7\n",
      "train 000 2.013119e-03 93.750000\n",
      "train 100 1.987861e-03 92.210705\n",
      "train 200 1.962137e-03 92.253576\n",
      "train 300 1.938678e-03 92.356208\n",
      "train 400 1.923363e-03 92.394015\n",
      "train acc 92.363333\n",
      "valid 000 2.065839e-03 89.062500\n",
      "valid acc 91.210000\n",
      "Epoch 8\n",
      "train 000 2.039345e-03 92.968750\n",
      "train 100 1.870658e-03 92.690285\n",
      "train 200 1.877096e-03 92.642257\n",
      "train 300 1.876243e-03 92.683243\n",
      "train 400 1.899174e-03 92.594685\n",
      "train acc 92.610000\n",
      "valid 000 1.749743e-03 93.750000\n",
      "valid acc 92.820000\n",
      "Epoch 9\n",
      "train 000 2.732409e-03 89.062500\n",
      "train 100 1.838328e-03 92.790842\n",
      "train 200 1.802986e-03 92.817164\n",
      "train 300 1.804171e-03 92.872716\n",
      "train 400 1.831200e-03 92.783666\n",
      "train acc 92.891667\n",
      "valid 000 1.604971e-03 94.531250\n",
      "valid acc 92.980000\n",
      "Epoch 10\n",
      "train 000 1.627521e-03 94.531250\n",
      "train 100 1.684697e-03 93.409653\n",
      "train 200 1.726241e-03 93.217506\n",
      "train 300 1.699068e-03 93.342504\n",
      "train 400 1.696910e-03 93.340867\n",
      "train acc 93.355000\n",
      "valid 000 1.692700e-03 92.968750\n",
      "valid acc 94.110000\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions needed for evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def random_weights(dim, mu=0, sigma=1):\n",
    "    # gaussian\n",
    "    return np.random.normal(mu, sigma, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_parameters(model, params_to_load):\n",
    "    lb = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        ub = lb + param.nelement()\n",
    "        layer_size = tuple(param.size())\n",
    "        param.data = (torch.from_numpy(params_to_load[lb:ub].reshape(layer_size))).type(torch.FloatTensor)\n",
    "        lb += param.nelement()\n",
    "    assert ub == len(params_to_load)\n",
    "    return  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_combination(iterable, sample_size):\n",
    "    \"\"\"Random selection from itertools.combinations(iterable, r).\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    \n",
    "    indices = sorted(random.sample(range(n), sample_size))\n",
    "    \n",
    "    return tuple(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# polynomial mutation from pymoo package\n",
    "from pymoo.model.mutation import Mutation\n",
    "\n",
    "class PolynomialMutation(Mutation):\n",
    "    def __init__(self, eta_mut, prob_mut=None, xl=-5, xu=5):\n",
    "        super().__init__()\n",
    "        # artificial lower&upper bound for weight\n",
    "        self.eta_mut = float(eta_mut)\n",
    "        self.xl = xl\n",
    "        self.xu = xu\n",
    "        if prob_mut is not None:\n",
    "            self.prob_mut = float(prob_mut)\n",
    "        else:\n",
    "            self.prob_mut = None\n",
    "        \n",
    "    def do(self, X):\n",
    "        \n",
    "        Y = np.full(X.shape, np.inf)\n",
    "\n",
    "        if self.prob_mut is None:\n",
    "            self.prob_mut = 1.0 / problem.n_var\n",
    "\n",
    "        do_mutation = np.random.rand(X.shape[0]) < self.prob_mut\n",
    "\n",
    "        Y[:] = X\n",
    "\n",
    "        xl = np.repeat(self.xl, X.shape[0], axis=0)[do_mutation]\n",
    "        xu = np.repeat(self.xu, X.shape[0], axis=0)[do_mutation]\n",
    "    \n",
    "        X = X[do_mutation]\n",
    "\n",
    "        delta1 = (X - xl) / (xu - xl)\n",
    "        delta2 = (xu - X) / (xu - xl)\n",
    "\n",
    "        mut_pow = 1.0 / (self.eta_mut + 1.0)\n",
    "\n",
    "        rand = np.random.rand(X.shape[0])\n",
    "        mask = rand <= 0.5\n",
    "        mask_not = np.logical_not(mask)\n",
    "\n",
    "        deltaq = np.zeros(X.shape)\n",
    "\n",
    "        xy = 1.0 - delta1\n",
    "        val = 2.0 * rand + (1.0 - 2.0 * rand) * (np.power(xy, (self.eta_mut + 1.0)))\n",
    "        d = np.power(val, mut_pow) - 1.0\n",
    "        deltaq[mask] = d[mask]\n",
    "\n",
    "        xy = 1.0 - delta2\n",
    "        val = 2.0 * (1.0 - rand) + 2.0 * (rand - 0.5) * (np.power(xy, (self.eta_mut + 1.0)))\n",
    "        d = 1.0 - (np.power(val, mut_pow))\n",
    "        deltaq[mask_not] = d[mask_not]\n",
    "\n",
    "        # mutated values\n",
    "        _Y = X + deltaq * (xu - xl)\n",
    "\n",
    "        # back in bounds if necessary (floating point issues)\n",
    "        _Y[_Y < xl] = xl[_Y < xl]\n",
    "        _Y[_Y > xu] = xu[_Y > xu]\n",
    "\n",
    "        # set the values for output\n",
    "        Y[do_mutation] = _Y\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate(pop, train_queue, criterion):\n",
    "    # every individual in population will only be evaluated on one mini-batch\n",
    "    # assuming mini-batch size = total_num_training_data / population size\n",
    "    pop_eval = []\n",
    "    best_loss = np.inf\n",
    "    for step, (inputs, targets) in enumerate(train_queue):\n",
    "        if step < len(pop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            model = Net().to(device)\n",
    "            load_parameters(model, pop[step][1])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets).item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct = predicted.eq(targets).sum().item()\n",
    "            acc = 100.*correct/len(inputs)\n",
    "            pop_eval.append((loss, pop[step][1], acc))\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return pop_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evo_main(generations=3000,\n",
    "             population_size=100,\n",
    "             tournament_size=10,\n",
    "             p_mut=0.05, eta_m=30):\n",
    "    # simply apply gaussian noise (zero mean and std 1) to all parameters\n",
    "    # ------------- main routine ------------------ #\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    batch_size = int(50000/population_size)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = Net()\n",
    "    n_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('# number of trainable parameters = {}'.format(n_params_trainable))\n",
    "    del model\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # initialization\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        weights = random_weights(n_params_trainable, sigma=0.01)\n",
    "        population.append((np.inf, weights, 0))\n",
    "    \n",
    "    # evaluation\n",
    "    population = evaluate(population, train_loader, criterion)\n",
    "    \n",
    "    elite_idx = np.argmin([x[0] for x in population])\n",
    "    print('train %03d %e %e %f'% (0, p_mut, \n",
    "                                  population[elite_idx][0],\n",
    "                                  population[elite_idx][-1]))\n",
    "    \n",
    "    # main loop of evolution\n",
    "    for gen in range(1, generations+1):\n",
    "        sample = random_combination(population, tournament_size)\n",
    "        # best from the sample becomes parent\n",
    "        tmp = sorted(sample, key=lambda i: i[0])\n",
    "                \n",
    "        winner, loser = tmp[0], tmp[-1]\n",
    "        \n",
    "        # update mutation probability - linear drop \n",
    "        p_mut = p_mut - (p_mut - 1/n_params_trainable)/generations\n",
    "        child = [(np.inf, PolynomialMutation(eta_mut=eta_m, \n",
    "                                             prob_mut=p_mut,\n",
    "                                             xl=-2, xu=2).do(winner[1]))]\n",
    "        child = evaluate(child, train_loader, criterion)\n",
    "        \n",
    "        # replace loser in population with child\n",
    "        remove_idx = [i for i in range(len(population)) \n",
    "                      if np.all(population[i][1] == loser[1])][0]\n",
    "                \n",
    "        population.pop(remove_idx)\n",
    "        population += child\n",
    "        if gen % 100 == 0:\n",
    "            elite_idx = np.argmin([x[0] for x in population])\n",
    "            print('train %03d %e %e %f'% (0, p_mut, \n",
    "                                          population[elite_idx][0],\n",
    "                                          population[elite_idx][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of trainable parameters = 1394\n",
      "train 000 1.000000e+00 2.302330e+00 10.120000\n",
      "train 000 9.672342e-01 2.299078e+00 12.160000\n",
      "train 000 9.355429e-01 2.293097e+00 10.080000\n",
      "train 000 9.048906e-01 2.282295e+00 10.160000\n",
      "train 000 8.752434e-01 2.273449e+00 10.360000\n",
      "train 000 8.465684e-01 2.273449e+00 10.360000\n",
      "train 000 8.188335e-01 2.273449e+00 10.360000\n",
      "train 000 7.920081e-01 2.273449e+00 10.360000\n",
      "train 000 7.660622e-01 2.272149e+00 19.440000\n",
      "train 000 7.409671e-01 2.272149e+00 19.440000\n",
      "train 000 7.166949e-01 2.264886e+00 12.400000\n",
      "train 000 6.932185e-01 2.257338e+00 18.240000\n",
      "train 000 6.705119e-01 2.257338e+00 18.240000\n",
      "train 000 6.485498e-01 2.257338e+00 18.240000\n",
      "train 000 6.273078e-01 2.247084e+00 12.720000\n",
      "train 000 6.067624e-01 2.231838e+00 18.320000\n",
      "train 000 5.868906e-01 2.225982e+00 20.240000\n",
      "train 000 5.676704e-01 2.212746e+00 14.240000\n",
      "train 000 5.490804e-01 2.212746e+00 14.240000\n",
      "train 000 5.311000e-01 2.212746e+00 14.240000\n",
      "train 000 5.137092e-01 2.212746e+00 14.240000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-13f38b0e12c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m evo_main(generations=3000, \n\u001b[1;32m      2\u001b[0m          \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          tournament_size=5, p_mut=1.0, eta_m=50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-38fce1768fc7>\u001b[0m in \u001b[0;36mevo_main\u001b[0;34m(generations, population_size, tournament_size, p_mut, eta_m)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                              \u001b[0mprob_mut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_mut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                              xl=-2, xu=2).do(winner[1]))]\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# replace loser in population with child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-19f5fdbaba62>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(pop, train_queue, criterion)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpop_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/external/miniconda3/envs/evodescent/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m     \"\"\"\n\u001b[0;32m-> 2508\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evo_main(generations=3000, \n",
    "         population_size=20, \n",
    "         tournament_size=5, p_mut=1.0, eta_m=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([8, 1, 3, 3])\n",
      "(72, 1)\n",
      "conv2.weight torch.Size([16, 8, 3, 3])\n",
      "(1152, 1)\n",
      "fc.weight torch.Size([10, 16])\n",
      "(160, 1)\n",
      "fc.bias torch.Size([10])\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# loop over model to print weights \n",
    "model = Net()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "    param_vector = param.view(param.nelement(), -1).data.numpy()\n",
    "    print(param_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394\n",
      "(1394,)\n",
      "lb = 0\n",
      "ub = 72\n",
      "lb = 72\n",
      "ub = 1224\n",
      "lb = 1224\n",
      "ub = 1384\n",
      "lb = 1384\n",
      "ub = 1394\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "n_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "init_guess = random_weights(n_params_trainable)\n",
    "\n",
    "lb = 0\n",
    "for name, param in model.named_parameters():\n",
    "    ub = lb + param.nelement()\n",
    "    print('lb = {}'.format(lb))\n",
    "    print('ub = {}'.format(ub))\n",
    "    layer_size = tuple(param.size())\n",
    "    param.data = torch.from_numpy(init_guess[lb:ub].reshape(layer_size))\n",
    "    lb += param.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
